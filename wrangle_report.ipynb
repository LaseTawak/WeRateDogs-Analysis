{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING REPORT\n",
    "### written by: Toluwalase Tawak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "----------------------------\n",
    "The goal of this project is to practice and familirise myself with data wrangling skills which include; Gathering, Assessment, Transformation and Cleaning. These activities will be carried out on the twitter archive of the [@RateDogs](https://twitter.com/dog_rates) account. This account rates people's dogs with a humorous comment about the dogs.  \n",
    "The report summarises how approached the data wrangling for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Details\n",
    "-----------------------------\n",
    "\n",
    "### Gathering Data\n",
    "The data used for this project consists of three different dataset. How these datasets were gathered are as follows:\n",
    "\n",
    "**`Twitter Archive`**: This data was provided by Udacity and was downloaded onto my local machine and uploaded into my Jupyter workspace in a virtual machine provided by Udacity. After importing the pandas library, the Twitter Archive data was read into a pandas dataframe using the **pandas.read_csv()** function.\n",
    "\n",
    "**`Tweet Image Prediction`**: A link was provided with the data [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv). I imported the Python requests library and use the the **get()** function from library to download the page into a variable.  \n",
    "\n",
    "Using the **with open()** function, I wrote the webiste content to a tsv file in the same working directory. I proceeded to read the downloaded tsv file into a pandas dataframe.\n",
    "\n",
    "**`Tweet Json`**: To carry out gathering of this data, a twitter developer account was required. While the approval for my developer account was taking time, I proceeded to use the data already gathered and stored by Udacity for students who might face difficulties with having their developer accounts approved. The data was downloaded unto my local machine from [here](https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt) as a txt file, and then uploaded to my Jupyter workspace in a virtual machine provided by Udacity.\n",
    "\n",
    "Using the **with open()** function again, I loaded each line of JSON format and appended the lines into list named **status**. This list was then fed as an argument to the **pandas.DataFrame()** function and converted into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data\n",
    "After gathering the datasets and creating DataFrame objects with them, I went on to assess them;\n",
    "\n",
    "1. I first of assessed each dataframe **visually** by printing them out and visually observing the content and structure of the three different dataframes individually. This was done in jupiter notebook by scrolled through each dataframe in every direction.\n",
    "\n",
    "2. I then went out to carry out **programmatic** assessments of the three dataframes using various pandas methods and functions such as **shape, info(), describe(), notnull(), head(), nunique(), sample(), duplicated(), value_counts(), query()** and **columns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "This section of the wrangling process was broken down into three parts:\n",
    ">1. `Define`: Where the cleaning process to be carried out was explained\n",
    ">2. `Code`: Code needed to achieve the cleaning goal defined was written and run.\n",
    ">3. `Test`: Code was written and run to confirm that the cleaning goal was achieved.\n",
    "\n",
    "\n",
    "To begin, copies of the three datasets were created. These copies were used to carry out the cleaning activities.  \n",
    "The cleaning proccesses carried out on each dataset are as follows:\n",
    "\n",
    "1. Non-null rows in the in_reply_to_status_id and retweeted_status_id columns were removed. This was to provide us with a dataset containing only original tweets and no retweets or replies.  \n",
    "  \n",
    "  \n",
    "2. Five columns, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id and in_reply_to_user_id columns were dropped because they now contained only null values.  \n",
    "  \n",
    "  \n",
    "3. The id column in Image Prediction was renamed to tweet_id. Values in the tweet_id columns for all three tables were converted from integer to string. The timestamp column in the archive dataframe was converted to datetime format.  \n",
    "  \n",
    "  \n",
    "4. The four columns containing stages of dogs being rated were concanated into one column (description) and cleaned to carry only one description for the dogs.  \n",
    "  \n",
    "  \n",
    "5. All rows whose the name attribute began with lower case were renamed and given the value None, the previous names were confirmed to not be actual names. Two names were renamed to match the complete names of the dogs.  \n",
    "  \n",
    "  \n",
    "6. Special characters in the text column were replaced with the appropriate strings and links in text column were replaced with empty strings.  \n",
    "  \n",
    "  \n",
    "7. Values in the p1, p2, and p3 columns cleaned by replacing underscores with a space and making the first letter of each word upper case.  \n",
    "  \n",
    "  \n",
    "8. p1, p2 and p3 was melted into one column. p1_conf, p2_conf, p3_conf was melted into one column. A function was created to keep the corresponding values for the first **True** value in either p1_dog, p2_dog or p3_dog.  \n",
    "  \n",
    "  \n",
    "9. The three dataframes were merged on their tweet_id column to become one dataframe.  \n",
    "  \n",
    "  \n",
    "10. Suspicious values in both rating columns were addressed by dropping rows were the tweets was rating more than one dog, correcting and rounding decimal ratings, rows with the wrong ratings and dropping rows with no rating in their text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data\n",
    "After gathering, assessing and cleaning, the merged dataset was converted and saved as a csv file called twitter_archive_master.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook wrangle_report.ipynb to markdown\n",
      "[NbConvertApp] Writing 6211 bytes to wrangle_report.md\n"
     ]
    }
   ],
   "source": [
    "# from subprocess import call\n",
    "# call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])\n",
    "!jupyter nbconvert --to markdown wrangle_report.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
